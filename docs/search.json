[
  {
    "objectID": "Homework5.html",
    "href": "Homework5.html",
    "title": "ST563 Homework 5",
    "section": "",
    "text": "The goal of this homework is to run two models from the notes.\n\n\n\n#load libraries.  Order seems to be important to their installation if they are not already installed in your system\n\n#install.packages(\"keras\")\nlibrary(keras)\n#install_keras() below is necessary to unpack tensorflow(), but doesn't need to be run in each iteration of this code once keras is unpackaged in the environment.\n#install_keras()\nlibrary(tensorflow)\nlibrary(caret)\nlibrary(tidyverse)\n#confirm tensorflow loaded\n#tf$constant(\"Hello TensorFlow!\")\n\n\n\n\nFirst, we’ll create a train and test split of the data and flatten.\n\n#load the data\nmnist &lt;- dataset_mnist()\n#set seed for reproducibility\nset.seed(10)\n#Prepare training dataset\ntrain_images &lt;- mnist$train$x %&gt;%\n  array_reshape(c(60000, 28 * 28))\ntrain_images &lt;- train_images / 255\ntrain_labels &lt;- mnist$train$y %&gt;%\n  to_categorical(10)\n\n#Prepare test dataset\ntest_images &lt;- mnist$test$x %&gt;%\n  array_reshape(c(10000, 28 * 28))\ntest_images &lt;- test_images / 255\ntest_labels &lt;- mnist$test$y %&gt;%\n  to_categorical(10)\n\nOkay, that seemed to at least be marginally functional. I tried to do this earlier and got errors there was not a valid version of tensorflow(). There are also objects containing data in the environment at this point."
  },
  {
    "objectID": "Homework5.html#modeling-implementation-problems",
    "href": "Homework5.html#modeling-implementation-problems",
    "title": "ST563 Homework 5",
    "section": "",
    "text": "The goal of this homework is to run two models from the notes.\n\n\n\n#load libraries.  Order seems to be important to their installation if they are not already installed in your system\n\n#install.packages(\"keras\")\nlibrary(keras)\n#install_keras() below is necessary to unpack tensorflow(), but doesn't need to be run in each iteration of this code once keras is unpackaged in the environment.\n#install_keras()\nlibrary(tensorflow)\nlibrary(caret)\nlibrary(tidyverse)\n#confirm tensorflow loaded\n#tf$constant(\"Hello TensorFlow!\")\n\n\n\n\nFirst, we’ll create a train and test split of the data and flatten.\n\n#load the data\nmnist &lt;- dataset_mnist()\n#set seed for reproducibility\nset.seed(10)\n#Prepare training dataset\ntrain_images &lt;- mnist$train$x %&gt;%\n  array_reshape(c(60000, 28 * 28))\ntrain_images &lt;- train_images / 255\ntrain_labels &lt;- mnist$train$y %&gt;%\n  to_categorical(10)\n\n#Prepare test dataset\ntest_images &lt;- mnist$test$x %&gt;%\n  array_reshape(c(10000, 28 * 28))\ntest_images &lt;- test_images / 255\ntest_labels &lt;- mnist$test$y %&gt;%\n  to_categorical(10)\n\nOkay, that seemed to at least be marginally functional. I tried to do this earlier and got errors there was not a valid version of tensorflow(). There are also objects containing data in the environment at this point."
  },
  {
    "objectID": "Homework5.html#two-layer-deep-learning-model",
    "href": "Homework5.html#two-layer-deep-learning-model",
    "title": "ST563 Homework 5",
    "section": "Two layer Deep Learning model",
    "text": "Two layer Deep Learning model\nBelow, we’ll try a two layer deep learning model.\n\n#define the layers and their activation funtions\nnetwork &lt;- keras_model_sequential() %&gt;%\n  layer_dense(units = 512, activation = \"relu\",\n              input_shape = c(28 * 28)) %&gt;%\n  layer_dense(units = 10, activation = \"softmax\")\n\nnetwork\n\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense_1 (Dense)                    (None, 512)                     401920      \n dense (Dense)                      (None, 10)                      5130        \n================================================================================\nTotal params: 407050 (1.55 MB)\nTrainable params: 407050 (1.55 MB)\nNon-trainable params: 0 (0.00 Byte)\n________________________________________________________________________________\n\n\nNow, we will compile and train the model.\n\n#compile model\nnetwork %&gt;% compile(\n  optimizer = optimizer_rmsprop(),\n  loss = \"categorical_crossentropy\",\n  metrics = c(\"accuracy\")\n)\n\n#train model\nhistory &lt;- network %&gt;%\n  fit(train_images, train_labels,\n      epochs = 15, batch_size = 128,\n      validation_split = 0.2)\n\nEpoch 1/15\n375/375 - 2s - loss: 0.2957 - accuracy: 0.9147 - val_loss: 0.1500 - val_accuracy: 0.9576 - 2s/epoch - 5ms/step\nEpoch 2/15\n375/375 - 1s - loss: 0.1215 - accuracy: 0.9647 - val_loss: 0.1106 - val_accuracy: 0.9691 - 1s/epoch - 3ms/step\nEpoch 3/15\n375/375 - 1s - loss: 0.0789 - accuracy: 0.9773 - val_loss: 0.0972 - val_accuracy: 0.9712 - 1s/epoch - 3ms/step\nEpoch 4/15\n375/375 - 1s - loss: 0.0568 - accuracy: 0.9833 - val_loss: 0.0872 - val_accuracy: 0.9727 - 1s/epoch - 3ms/step\nEpoch 5/15\n375/375 - 1s - loss: 0.0423 - accuracy: 0.9871 - val_loss: 0.0782 - val_accuracy: 0.9764 - 1s/epoch - 3ms/step\nEpoch 6/15\n375/375 - 1s - loss: 0.0328 - accuracy: 0.9907 - val_loss: 0.0737 - val_accuracy: 0.9786 - 1s/epoch - 3ms/step\nEpoch 7/15\n375/375 - 1s - loss: 0.0245 - accuracy: 0.9930 - val_loss: 0.0745 - val_accuracy: 0.9795 - 1s/epoch - 3ms/step\nEpoch 8/15\n375/375 - 1s - loss: 0.0177 - accuracy: 0.9955 - val_loss: 0.0781 - val_accuracy: 0.9767 - 1s/epoch - 3ms/step\nEpoch 9/15\n375/375 - 1s - loss: 0.0133 - accuracy: 0.9966 - val_loss: 0.0810 - val_accuracy: 0.9785 - 1s/epoch - 3ms/step\nEpoch 10/15\n375/375 - 1s - loss: 0.0098 - accuracy: 0.9978 - val_loss: 0.0775 - val_accuracy: 0.9789 - 1s/epoch - 3ms/step\nEpoch 11/15\n375/375 - 1s - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0763 - val_accuracy: 0.9811 - 1s/epoch - 3ms/step\nEpoch 12/15\n375/375 - 1s - loss: 0.0055 - accuracy: 0.9987 - val_loss: 0.0769 - val_accuracy: 0.9803 - 1s/epoch - 3ms/step\nEpoch 13/15\n375/375 - 1s - loss: 0.0041 - accuracy: 0.9992 - val_loss: 0.0807 - val_accuracy: 0.9791 - 1s/epoch - 3ms/step\nEpoch 14/15\n375/375 - 1s - loss: 0.0027 - accuracy: 0.9996 - val_loss: 0.0805 - val_accuracy: 0.9813 - 1s/epoch - 3ms/step\nEpoch 15/15\n375/375 - 1s - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.0850 - val_accuracy: 0.9804 - 1s/epoch - 4ms/step\n\nplot(history)\n\n\n\n\n\n\n\n\nNext, we’ll plot a confusion matrix and the overall accuracy. First, the Confusion Matrix.\n\n#it seems that \"predict_classes()\" has been removed from tensorflow, the following code was taken from https://tensorflow.rstudio.com/reference/keras/predict_proba\npred &lt;- network %&gt;% \n  predict(test_images) %&gt;% k_argmax()\n\n313/313 - 0s - 385ms/epoch - 1ms/step\n\n#str(pred)\n#head(pred)\n#str(pred$numpy())\n#str(mnist$test$y)\n#str(as.factor(mnist$test$y))\n#str(pred)\nconf_mat &lt;- table(as.factor(pred$numpy()), as.factor(mnist$test$y))\nnames(dimnames(conf_mat)) &lt;- c(\"Predicted\", \"Actual\")\nconf_mat\n\n         Actual\nPredicted    0    1    2    3    4    5    6    7    8    9\n        0  970    0    3    1    2    4    4    1    4    3\n        1    1 1129    2    0    0    0    3    5    2    4\n        2    1    2 1010    1    2    0    1    5    4    0\n        3    0    1    2  994    1    8    1    2    5    2\n        4    1    0    3    0  962    1    1    0    4    7\n        5    1    1    0    3    0  869    2    0    5    3\n        6    3    1    2    0    4    4  946    0    3    0\n        7    1    0    7    5    2    1    0 1008    4    3\n        8    1    1    3    3    0    2    0    3  939    0\n        9    1    0    0    3    9    3    0    4    4  987\n\n\nNow, the accuracy\n\naccuracy &lt;- sum(diag(conf_mat))/sum(conf_mat)\naccuracyPct &lt;- accuracy*100\n\nThe model predictions from the test dataset result in 98.14 % accuracy. This looks pretty good compared to models that I have trained before in this class."
  },
  {
    "objectID": "Homework5.html#model-2-multinomial-logistic-regression",
    "href": "Homework5.html#model-2-multinomial-logistic-regression",
    "title": "ST563 Homework 5",
    "section": "Model 2, Multinomial Logistic Regression",
    "text": "Model 2, Multinomial Logistic Regression\nWe’ll first start with the code from the prompt that defines the model.\n\n#set a seed for reproducibility\nset.seed(10)\n#multnomial logit regression\nmlogit &lt;- keras_model_sequential() %&gt;%\n  layer_dense(input_shape = 28*28,\n              units = 10, activation = \"softmax\")\n\n#compile network\nmlogit %&gt;% compile(\n  optimizer = optimizer_rmsprop(),\n  loss = \"categorical_crossentropy\",\n  metrics = c(\"accuracy\")\n)\n\n#train the model\nhistoryLogit &lt;- mlogit %&gt;%\n  fit(train_images, train_labels,\n      epochs = 15, batch_size = 128, \n      validation_split = 0.2)\n\nEpoch 1/15\n375/375 - 1s - loss: 0.6554 - accuracy: 0.8410 - val_loss: 0.3602 - val_accuracy: 0.9049 - 1s/epoch - 3ms/step\nEpoch 2/15\n375/375 - 1s - loss: 0.3529 - accuracy: 0.9031 - val_loss: 0.3081 - val_accuracy: 0.9162 - 617ms/epoch - 2ms/step\nEpoch 3/15\n375/375 - 1s - loss: 0.3179 - accuracy: 0.9119 - val_loss: 0.2929 - val_accuracy: 0.9193 - 599ms/epoch - 2ms/step\nEpoch 4/15\n375/375 - 1s - loss: 0.3018 - accuracy: 0.9165 - val_loss: 0.2833 - val_accuracy: 0.9212 - 613ms/epoch - 2ms/step\nEpoch 5/15\n375/375 - 1s - loss: 0.2925 - accuracy: 0.9186 - val_loss: 0.2788 - val_accuracy: 0.9233 - 606ms/epoch - 2ms/step\nEpoch 6/15\n375/375 - 1s - loss: 0.2855 - accuracy: 0.9201 - val_loss: 0.2741 - val_accuracy: 0.9264 - 602ms/epoch - 2ms/step\nEpoch 7/15\n375/375 - 1s - loss: 0.2807 - accuracy: 0.9216 - val_loss: 0.2718 - val_accuracy: 0.9272 - 603ms/epoch - 2ms/step\nEpoch 8/15\n375/375 - 1s - loss: 0.2765 - accuracy: 0.9231 - val_loss: 0.2708 - val_accuracy: 0.9262 - 605ms/epoch - 2ms/step\nEpoch 9/15\n375/375 - 1s - loss: 0.2735 - accuracy: 0.9240 - val_loss: 0.2693 - val_accuracy: 0.9269 - 607ms/epoch - 2ms/step\nEpoch 10/15\n375/375 - 1s - loss: 0.2712 - accuracy: 0.9248 - val_loss: 0.2674 - val_accuracy: 0.9268 - 613ms/epoch - 2ms/step\nEpoch 11/15\n375/375 - 1s - loss: 0.2684 - accuracy: 0.9254 - val_loss: 0.2658 - val_accuracy: 0.9278 - 617ms/epoch - 2ms/step\nEpoch 12/15\n375/375 - 1s - loss: 0.2663 - accuracy: 0.9267 - val_loss: 0.2644 - val_accuracy: 0.9285 - 600ms/epoch - 2ms/step\nEpoch 13/15\n375/375 - 1s - loss: 0.2646 - accuracy: 0.9263 - val_loss: 0.2646 - val_accuracy: 0.9281 - 602ms/epoch - 2ms/step\nEpoch 14/15\n375/375 - 1s - loss: 0.2631 - accuracy: 0.9275 - val_loss: 0.2643 - val_accuracy: 0.9302 - 606ms/epoch - 2ms/step\nEpoch 15/15\n375/375 - 1s - loss: 0.2616 - accuracy: 0.9281 - val_loss: 0.2635 - val_accuracy: 0.9302 - 617ms/epoch - 2ms/step\n\nplot(historyLogit)\n\n\n\n\n\n\n\n\nThe multinomial logit regression doesn’t look as good as the two layer deep learning model from prompt one. Let’s plot the confusion matrix as evalute the accuracy to see a direct comparison on the test dataset.\n\npredLogit &lt;- mlogit %&gt;% \n  predict(test_images) %&gt;% k_argmax()\n\n313/313 - 0s - 281ms/epoch - 899us/step\n\nconf_mat_logit &lt;- table(as.factor(predLogit$numpy()), as.factor(mnist$test$y))\nnames(dimnames(conf_mat_logit)) &lt;- c(\"Predicted\", \"Actual\")\nconf_mat_logit\n\n         Actual\nPredicted    0    1    2    3    4    5    6    7    8    9\n        0  966    0    7    3    2    9   11    1    7   12\n        1    0 1117    9    0    2    3    3    7    8    7\n        2    1    3  922   20    4    5    6   21    7    1\n        3    1    2   17  922    1   35    1    6   21   12\n        4    0    0    7    0  921   10    8    9    9   27\n        5    5    1    3   21    0  766   12    0   22    5\n        6    4    4   12    3    9   16  914    0   10    0\n        7    2    2   10   12    3    7    1  954   11   22\n        8    1    6   41   22    8   33    2    1  874    4\n        9    0    0    4    7   32    8    0   29    5  919\n\n\nNow, the accuracy of the multinomial logit regression:\n\naccuracyLogit &lt;- sum(diag(conf_mat_logit))/sum(conf_mat_logit)\naccuracyPctLogit &lt;- accuracyLogit*100\n\nThe model predictions from the test dataset result in 92.75 % accuracy. This is quite a bit lower than the accuracy of the two layer deep learning model."
  }
]